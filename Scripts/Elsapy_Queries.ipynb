{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d5df3d",
   "metadata": {},
   "source": [
    "# Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3207a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell once per environment\n",
    "!pip install https://github.com/ElsevierDev/elsapy/archive/master.zip\n",
    "!pip install elsapy\n",
    "!pip install xmltodict\n",
    "!pip install pubchempy\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aebad13",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elsevier Libraries\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "# Libraries for URL access & basic functionality\n",
    "import json\n",
    "import csv\n",
    "import pprint\n",
    "import requests\n",
    "import xmltodict\n",
    "import urllib3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import os\n",
    "import itertools \n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import urllib.parse\n",
    "# Import warnings library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# xml.etree.ElementTree â€” The ElementTree XML API\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e84d2",
   "metadata": {},
   "source": [
    "# API Key & Institution Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append \"apikey\" and \"insttoken\" as suggested in the ElsSearch document into a config file\n",
    "CONFIG = {\"apikey\": \"YOUR_API_KEY\", \n",
    "          \"insttoken\": \"YOUR_INST_TOKEN\"}\n",
    "API_KEY = CONFIG[\"apikey\"]\n",
    "INST_TOKEN = CONFIG[\"insttoken\"]\n",
    "client = ElsClient(API_KEY)\n",
    "client.inst_token = INST_TOKEN\n",
    "\n",
    "# Maximum of 6000 searches are returned on calling the ElsSearch API\n",
    "# Create a num array of 0 to 5900 spaces 100 apart per page.\n",
    "# Each page of Search returns 100 entries until 6000 is reached.\n",
    "NUM = np.linspace(0, 5900, 60, dtype = int)\n",
    "\n",
    "QUERY_PATH = (\"D:/MS_Thesis/API_Queries/\")\n",
    "os.makedirs(QUERY_PATH, exist_ok = True) \n",
    "os.path.exists(QUERY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253e8a8",
   "metadata": {},
   "source": [
    "# Lists for Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760eb3e",
   "metadata": {},
   "source": [
    "## Plastic polymers, plastic type, recycling technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a610b-0611-40de-8cb9-e22390c6d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polymer types, sorted by recycling category\n",
    "category1 = [\"polyethylene terephthalate\", \"pet\"]\n",
    "category2 = [\"high-density polyethylene\", \"high density polyethylene\", \"hdpe\"]\n",
    "category3 = [\"polyvinyl chloride\", \"pvc\"]\n",
    "category4 = [\"low-density polyethylene\", \"low density polyethylene\", \"ldpe\"]\n",
    "category5 = [\"polypropylene\", \"pp\"]\n",
    "category6 = [\"polystyrene\", \"styrene\", \"PS\"]\n",
    "category7 = [\"polyurethane\",\"PUR\", \"acrylonitrile butadiene styrene\", \"ABS\", \n",
    "             \"polyacrylate\", \"acrylic\", \"polycarbonate\", \"PC\", \"nylon\",\n",
    "             \"polylactic acid\", \"biodegradable plastic\", \"bio-plastic\", \n",
    "             \"bioplastic\", \"bioplastics\", \"polymethyl methacrylate\", \"PMMA\", \n",
    "             \"polytetrafluoroethylene\", \"teflon\", \"polyimide\", \"polysulfone\", \n",
    "             \"polyethersulfone\", \"polyarylsulfone\", \"polyphenylene sulfide\"]\n",
    "# Plastic product types\n",
    "plastic_types = [\"films\", \"film\", \"bottle\", \"bottles\", \"label\", \"labels\", \n",
    "                 \"container\", \"containers\", \"wrap\", \"wrapper\", \"bag\", \"bags\",\n",
    "                 \"multi-layer\", \"multi-layers\", \"multilayer\", \"multilayers\",\n",
    "                 \"multimaterial\", \"multimaterials\", \"multi-materials\",\n",
    "                 \"multi-material\", \"sack\", \"sacks\", \"cap\", \"caps\", \"lid\", \"lids\",\n",
    "                 \"coating\", \"coatings\", \"basket\", \"baskets\", \"tray\", \"trays\"]\n",
    "# All polymer types into a single list\n",
    "all_polymers = list(itertools.chain(category1, category2, category3, category4, \n",
    "                                    category5, category6, category7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922daa50",
   "metadata": {},
   "source": [
    "#  Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XML_DOI(link):\n",
    "    \"\"\"\n",
    "    Fetch XML metadata for given DOI URL using Elsevier API & save it to disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    link : str\n",
    "        Full URL of DOI endpoint that returns XML metadata.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Writes XML response bytes to loca lfile named \"doi.xml\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Header dictionary to authenticate & request XML from Elsevier API\n",
    "    headers_dict = {\"X-ELS-APIKey\": \"YOUR_API_KEY\", \n",
    "                    \"X-ELS-Insttoken\": \"YOUR_INST_TOKEN\", \n",
    "                    \"Accept\": \"application/xml\"}\n",
    "    \n",
    "    # Send GET request to DOI link\n",
    "    x = requests.get(link, headers = headers_dict) # x takes response of the HTTP request, passes link\n",
    "\n",
    "    # Save raw XML content to local file\n",
    "    with open(\"doi.xml\", \"wb\") as f:\n",
    "        f.write(x.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb72a9",
   "metadata": {},
   "source": [
    "# QUERY: Plastic {plastic_type} (Life Cycle Analysis OR Life Cycle Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in plastic_types:\n",
    "    # Stores the URLs (links) generated for querying the API as strings\n",
    "    # Used to make API requests in subsequent loop\n",
    "    # Reinitialized for each iteration in plastic_type_list (i.e., for each word in plastic_type_list)\n",
    "    # Loop to iterate over all papers\n",
    "    # UPI query can take many arguments (ex. start, count, & query)\n",
    "    link_list = []   \n",
    "    \n",
    "    # Build raw query with Boolean logic, see Elsevier documentation!\n",
    "    raw_query1 = \"'plastic' AND '{}' AND ('life cycle analysis' OR 'life cycle assessment')\".format(word)\n",
    "    encoded_query1 = urllib.parse.quote(raw_query1)\n",
    "    \n",
    "    # Loop to create and store the links for each word in plastic_type_list:\n",
    "    for i in range(0, 60):\n",
    "        start = \"https://api.elsevier.com/content/search/sciencedirect?\"\n",
    "        count = \"start=\" + str(NUM[i]) + \"&count=100\"\n",
    "        query_part = f\"&query={encoded_query1}\"\n",
    "        auth = f\"&apiKey={API_KEY}&insttoken={INST_TOKEN}\"\n",
    "        full_url = start + count + query_part + auth\n",
    "        link_list.append(full_url)\n",
    "        \n",
    "    # FOR DEBUGGING\n",
    "    test_url = link_list[0]\n",
    "    response = requests.get(test_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check for errors\n",
    "    if \"service-error\" in data:\n",
    "        print(f\"Error for '{word}':\", data[\"service-error\"][\"status\"][\"statusText\"])\n",
    "    else:\n",
    "        total_results = int(data.get(\"search-results\", {}).get(\"opensearch:totalResults\", 0))\n",
    "        print(f\"'{word}': {total_results} results found\")\n",
    "    \n",
    "    # Loop to fetch XML data for each link in link_list:\n",
    "    for j in tqdm(range(0, 60)): # Provides a visualization of the loop's progress via a progress bar\n",
    "        \n",
    "        # Debugging\n",
    "        response = requests.get(link_list[j])\n",
    "        # print(\"Querying:\", link_list[j])\n",
    "        # print(\"API Response:\", response.text[:500])  # Print first 500 characters to avoid huge output\n",
    "\n",
    "        xmlfile = XML_DOI(link_list[j])  # Continue with function...\n",
    "        # Read the data\n",
    "        tree = ET.parse(r\"doi.xml\")\n",
    "        root = tree.getroot()\n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            info_dict = {} # Dictionary to store the information from the queries (metadata)\n",
    "            \"\"\"\n",
    "            CHECK CODE\n",
    "            url = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}url').text\n",
    "            title = entry.find('{http://purl.org/dc/elements/1.1/}title').text\n",
    "            pub_name = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}publicationName').text\n",
    "            doi = entry.find('{http://prismstandard.org/namespaces/basic/2.0/}doi').text\n",
    "            #description = entry.find('{http://purl.org/dc/elements/1.1/}description').text\n",
    "            print(url, title, pub_name, doi)\n",
    "            print('\\n')\n",
    "            \"\"\"\n",
    "            # Extract the metadata:\n",
    "            info_dict[\"URL\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}url\").text\n",
    "            info_dict[\"Title\"] = entry.find(\"{http://purl.org/dc/elements/1.1/}title\").text\n",
    "            info_dict[\"Pub_Name\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}publicationName\").text\n",
    "            doi = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}doi\")\n",
    "            if doi is None:\n",
    "                info_dict[\"DOI\"] = None\n",
    "            else:\n",
    "                info_dict[\"DOI\"] = doi.text\n",
    "            # Write the URL, title, publication name, & DOI to info_list, i.e., metadata storage\n",
    "            info_list.append(info_dict) \n",
    "            \n",
    "    print(\"Total number of papers:\",len(info_list))\n",
    "\n",
    "    DOI =  [] # Initializing empty list to store DOI's\n",
    "    Title = [] # Initializing empty list to store titles\n",
    "    Pub_name = [] # Initializing empty list to store publication names\n",
    "    \n",
    "    # Loop over info_list to extract DOI, Title, & publication name & store them in previously initialized lists:\n",
    "    for i in range(len(info_list)): \n",
    "        DOI.append(info_list[i][\"DOI\"])\n",
    "        Title.append(info_list[i][\"Title\"])\n",
    "        Pub_name.append(info_list[i][\"Pub_Name\"])\n",
    "    \n",
    "    # Define the data frame to store the metadata for each query in plastic_type_list:\n",
    "    df =  pd.DataFrame()\n",
    "    df[\"Title\"] =  Title\n",
    "    df[\"Pub_name\"] = Pub_name\n",
    "    df[\"DOI\"] = DOI\n",
    "    \n",
    "    # Create string1 and string2 to join doi with institoken to make a single URL\n",
    "    string1 = \"https://api.elsevier.com/content/article/doi/\" # Base URL to access articles via DOI\n",
    "    string2 = \"?apiKey=YOUR_API_KEY&insttoken=YOUR_INST_TOKEN\" # Query string w/ API key & institution token\n",
    "    \n",
    "    # New column 'Link' that joins string 1 and string 2 to each DOI:\n",
    "    df[\"Link\"] = df[\"DOI\"].apply(lambda x: string1 + str(x) + string2)\n",
    "    \n",
    "    df.to_parquet(f\"{QUERY_PATH}/Plastic_Product_LCA/plastic_{word}_LCA_links.gzip\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0730b02",
   "metadata": {},
   "source": [
    "# QUERY: Recycling Technology OR Recycle Plastic {plastic_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb423f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in plastic_types:\n",
    "    link_list = []     \n",
    "    # Build raw query with Boolean logic, see Elsevier documentation!\n",
    "    raw_query2 = \"('recycling technology' OR 'recycle') AND 'plastic' AND '{}'\".format(word)\n",
    "    encoded_query2 = urllib.parse.quote(raw_query2)\n",
    "    \n",
    "    for i in range(0, 60):\n",
    "        start = \"https://api.elsevier.com/content/search/sciencedirect?\"\n",
    "        count = \"start=\" + str(NUM[i]) + \"&count=100\"\n",
    "        query_part = f\"&query={encoded_query2}\"\n",
    "        auth = f\"&apiKey={API_KEY}&insttoken={INST_TOKEN}\"\n",
    "        full_url = start + count + query_part + auth\n",
    "        link_list.append(full_url)\n",
    "    \n",
    "    # Debugging\n",
    "    test_url = link_list[0]\n",
    "    response = requests.get(test_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check for errors\n",
    "    if \"service-error\" in data:\n",
    "        print(f\"Error for '{word}':\", data[\"service-error\"][\"status\"][\"statusText\"])\n",
    "    else:\n",
    "        total_results = int(data.get(\"search-results\", {}).get(\"opensearch:totalResults\", 0))\n",
    "        print(f\"'{word}': {total_results} results found\")\n",
    "    \n",
    "    # Loop to fetch XML data for each link in link_list:\n",
    "    for j in tqdm(range(0, 60)):\n",
    "        \n",
    "        # Debugging\n",
    "        response = requests.get(link_list[j])\n",
    "        #print(\"Querying:\", link_list[j])\n",
    "        #print(\"API Response:\", response.text[:500])\n",
    "\n",
    "        xmlfile = XML_DOI(link_list[j])\n",
    "        # Read the data\n",
    "        tree = ET.parse(r\"doi.xml\")\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            info_dict = {} # Dictionary to store the information from the queries (metadata)\n",
    "            # Extract the metadata:\n",
    "            info_dict[\"URL\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}url\").text\n",
    "            info_dict[\"Title\"] = entry.find(\"{http://purl.org/dc/elements/1.1/}title\").text\n",
    "            info_dict[\"Pub_Name\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}publicationName\").text\n",
    "            doi = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}doi\")\n",
    "            if doi is None:\n",
    "                info_dict[\"DOI\"] = None\n",
    "            else:\n",
    "                info_dict[\"DOI\"] = doi.text\n",
    "            # Write the URL, title, publication name, & DOI to info_list, i.e., metadata storage\n",
    "            info_list.append(info_dict) \n",
    "    print(\"Total number of papers:\",len(info_list))\n",
    "\n",
    "    DOI =  [] # Initializing empty list to store DOI's\n",
    "    Title = [] # Initializing empty list to store titles\n",
    "    Pub_name = [] # Initializing empty list to store publication names\n",
    "    \n",
    "    # Loop over info_list to extract DOI, Title, & publication name & store them in previously initialized lists:\n",
    "    for i in range(len(info_list)): \n",
    "        DOI.append(info_list[i][\"DOI\"])\n",
    "        Title.append(info_list[i][\"Title\"])\n",
    "        Pub_name.append(info_list[i][\"Pub_Name\"])\n",
    "    \n",
    "    # Define the data frame to store the metadata for each query in plastic_type_list:\n",
    "    df =  pd.DataFrame()\n",
    "    df[\"Title\"] =  Title\n",
    "    df[\"Pub_name\"] = Pub_name\n",
    "    df[\"DOI\"] = DOI\n",
    "    \n",
    "    # Create string1 and string2 to join doi with institoken to make a single URL\n",
    "    string1 = \"https://api.elsevier.com/content/article/doi/\" # Base URL to access articles via DOI\n",
    "    string2 = \"?apiKey=YOUR_API_KEY&insttoken=YOUR_INST_TOKEN\" # Query string w/ API key & institution token\n",
    "    \n",
    "    # New column 'Link' that joins string 1 and string 2 to each DOI:\n",
    "    df[\"Link\"] = df[\"DOI\"].apply(lambda x: string1 + str(x) + string2)\n",
    "    \n",
    "    df.to_parquet(f\"{QUERY_PATH}/Recycle_Plastic_Products/recycle_plastic_{word}_links.gzip\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c892b",
   "metadata": {},
   "source": [
    "# QUERY: {polymer type} Life Cycle Analysis OR Life Cycle Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in all_polymers:\n",
    "    link_list = []     \n",
    "    raw_query3 = \"'{}' AND ('life cycle analysis' OR 'life cycle assessment')\".format(word)\n",
    "    encoded_query3 = urllib.parse.quote(raw_query3)\n",
    "    info_list = []\n",
    "\n",
    "    for i in range(0, 60):\n",
    "        start = \"https://api.elsevier.com/content/search/sciencedirect?\"\n",
    "        count = \"start=\" + str(NUM[i]) + \"&count=100\"\n",
    "        query_part = f\"&query={encoded_query3}\"\n",
    "        auth = f\"&apiKey={API_KEY}&insttoken={INST_TOKEN}\"\n",
    "        full_url = start + count + query_part + auth\n",
    "        link_list.append(full_url)\n",
    "        \n",
    "    # Debugging\n",
    "    test_url = link_list[0]\n",
    "    response = requests.get(test_url)\n",
    "    data = response.json()\n",
    "    # Check for errors\n",
    "    if \"service-error\" in data:\n",
    "        print(f\"Error for '{word}':\", data[\"service-error\"][\"status\"][\"statusText\"])\n",
    "    else:\n",
    "        total_results = int(data.get(\"search-results\", {}).get(\"opensearch:totalResults\", 0))\n",
    "        print(f\"'{word}': {total_results} results found\")\n",
    "    \n",
    "    # Loop to fetch XML data for each link in link_list:\n",
    "    for j in tqdm(range(0, 60)): #Provides a visualization of the loop's progress via a progress bar\n",
    "        \n",
    "        # Debugging\n",
    "        response = requests.get(link_list[j])\n",
    "        #print(\"Querying:\", link_list[j])\n",
    "        #print(\"API Response:\", response.text[:500])  # Print first 500 characters to avoid huge output\n",
    "        \n",
    "        xmlfile = XML_DOI(link_list[j])\n",
    "        # Read the data\n",
    "        tree = ET.parse(r\"doi.xml\")\n",
    "        root = tree.getroot()\n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            info_dict = {} # Dictionary to store the information from the queries (metadata)\n",
    "            # Extracting the metadata:\n",
    "            # Extracting the metadata:\n",
    "            info_dict[\"URL\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}url\").text\n",
    "            info_dict[\"Title\"] = entry.find(\"{http://purl.org/dc/elements/1.1/}title\").text\n",
    "            info_dict[\"Pub_Name\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}publicationName\").text\n",
    "            doi = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}doi\")\n",
    "            if doi is None:\n",
    "                info_dict[\"DOI\"] = None\n",
    "            else:\n",
    "                info_dict[\"DOI\"] = doi.text\n",
    "            info_list.append(info_dict) # Writing the URL, title, publication name, & DOI to info_list, i.e., metadata storage\n",
    "    print(\"Total number of papers:\",len(info_list))\n",
    "\n",
    "    DOI =  [] # Initializing empty list to store DOI's\n",
    "    Title = [] # Initializing empty list to store titles\n",
    "    Pub_name = [] # Initializing empty list to store publication names\n",
    "    \n",
    "    # Loop over info_list to extract DOI, Title, & publication name \n",
    "    # & store them in previously initialized lists:\n",
    "    for i in range(len(info_list)): \n",
    "        DOI.append(info_list[i][\"DOI\"])\n",
    "        Title.append(info_list[i][\"Title\"])\n",
    "        Pub_name.append(info_list[i][\"Pub_Name\"])\n",
    "    \n",
    "    # Define the data frame to store the metadata for each query in plastic_type_list:\n",
    "    df =  pd.DataFrame()\n",
    "    df[\"Title\"] =  Title\n",
    "    df[\"Pub_name\"] = Pub_name\n",
    "    df[\"DOI\"] = DOI\n",
    "    \n",
    "    # Create string1 and string2 to join doi with institoken to make a single URL\n",
    "    string1 = \"https://api.elsevier.com/content/article/doi/\" # Base URL to access articles via DOI\n",
    "    string2 = \"?apiKey=YOUR_API_KEY&insttoken=YOUR_INST_TOKEN\" # Query string w/ API key & institution token\n",
    "    \n",
    "    # New column 'Link' that joins string 1 and string 2 to each DOI:\n",
    "    df[\"Link\"] = df[\"DOI\"].apply(lambda x: string1 + str(x) + string2)\n",
    "    df.to_parquet(f\"{QUERY_PATH}/Polymer_LCA/{word}_LCA_links.gzip\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415eb99b",
   "metadata": {},
   "source": [
    "# QUERY: Recycling Technology OR Recycle {polymer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7c38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in all_polymers:\n",
    "    link_list = []     \n",
    "    raw_query4 =\"('recycling technology' OR 'recycle') AND '{}'\".format(word)\n",
    "    encoded_query4 = urllib.parse.quote(raw_query4)\n",
    "    info_list = []\n",
    "\n",
    "    for i in range(0, 60):\n",
    "        start = \"https://api.elsevier.com/content/search/sciencedirect?\"\n",
    "        count = \"start=\" + str(NUM[i]) + \"&count=100\"\n",
    "        query_part = f\"&query={encoded_query4}\"\n",
    "        auth = f\"&apiKey={API_KEY}&insttoken={INST_TOKEN}\"\n",
    "        full_url = start + count + query_part + auth\n",
    "        link_list.append(full_url)\n",
    "        \n",
    "    # Debugging\n",
    "    test_url = link_list[0]\n",
    "    response = requests.get(test_url)\n",
    "    data = response.json()\n",
    "    # Check for errors\n",
    "    if \"service-error\" in data:\n",
    "        print(f\"Error for '{word}':\", data[\"service-error\"][\"status\"][\"statusText\"])\n",
    "    else:\n",
    "        total_results = int(data.get(\"search-results\", {}).get(\"opensearch:totalResults\", 0))\n",
    "        print(f\"'{word}': {total_results} results found\")\n",
    "    \n",
    "    # Loop to fetch XML data for each link in link_list:\n",
    "    for j in tqdm(range(0, 60)): #Provides a visualization of the loop's progress via a progress bar\n",
    "        \n",
    "        # Debugging\n",
    "        response = requests.get(link_list[j])\n",
    "        #print(\"Querying:\", link_list[j])\n",
    "        #print(\"API Response:\", response.text[:500])  # Print first 500 characters to avoid huge output\n",
    "\n",
    "        xmlfile = XML_DOI(link_list[j])\n",
    "        # Read the data\n",
    "        tree = ET.parse(r\"doi.xml\")\n",
    "        root = tree.getroot()\n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            info_dict = {} # Dictionary to store the information from the queries (metadata)\n",
    "            # Extracting the metadata:\n",
    "            info_dict[\"URL\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}url\").text\n",
    "            info_dict[\"Title\"] = entry.find(\"{http://purl.org/dc/elements/1.1/}title\").text\n",
    "            info_dict[\"Pub_Name\"] = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}publicationName\").text\n",
    "            doi = entry.find(\"{http://prismstandard.org/namespaces/basic/2.0/}doi\")\n",
    "            if doi is None:\n",
    "                info_dict[\"DOI\"] = None\n",
    "            else:\n",
    "                info_dict[\"DOI\"] = doi.text\n",
    "            # Write the URL, title, publication name, & DOI to info_list, i.e., metadata storage\n",
    "            info_list.append(info_dict) \n",
    "    print(\"Total number of papers:\",len(info_list))\n",
    "\n",
    "    DOI =  [] # Initializing empty list to store DOI's\n",
    "    Title = [] # Initializing empty list to store titles\n",
    "    Pub_name = [] # Initializing empty list to store publication names\n",
    "    \n",
    "    # Loop over info_list to extract DOI, Title, & publication name \n",
    "    # & store them in previously initialized lists:\n",
    "    for i in range(len(info_list)): \n",
    "        DOI.append(info_list[i][\"DOI\"])\n",
    "        Title.append(info_list[i][\"Title\"])\n",
    "        Pub_name.append(info_list[i][\"Pub_Name\"])\n",
    "    \n",
    "    # Define the data frame to store the metadata for each query in plastic_type_list:\n",
    "    df =  pd.DataFrame()\n",
    "    df[\"Title\"] =  Title\n",
    "    df[\"Pub_name\"] = Pub_name\n",
    "    df[\"DOI\"] = DOI\n",
    "\n",
    "    string1 = \"https://api.elsevier.com/content/article/doi/\" # Base URL to access articles via DOI\n",
    "    string2 = \"?apiKey=YOUR_API_KEY&insttoken=YOUR_INST_TOKEN\" # Query string w/ API key & institution token\n",
    "    \n",
    "    # New column 'Link' that joins string 1 and string 2 to each DOI:\n",
    "    df['Link'] = df['DOI'].apply(lambda x: string1 + str(x) + string2)\n",
    "    df.to_parquet(f\"{QUERY_PATH}/Recycle_Polymers/recycle_{word}_links.gzip\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048af30",
   "metadata": {},
   "source": [
    "# Combining queries into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477cfe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query_files = .gzip file that contains all papers produced from a particular query\n",
    "query_files = os.listdir(QUERY_PATH)\n",
    "len(query_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09872f5b-c1a3-4854-aaf3-0a78eefbb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all .gzip files recursively in subdirectories\n",
    "gzip_files = glob.glob(os.path.join(QUERY_PATH, \"**/*.gzip\"), recursive = True)\n",
    "\n",
    "# Read and concatenate all DataFrames\n",
    "df_list = [pd.read_parquet(file) for file in gzip_files]\n",
    "combined_df = pd.concat(df_list, ignore_index = True)\n",
    "\n",
    "# Before saving to csv, clean up dataframe\n",
    "combined_df.dropna(subset=[\"Title\", \"DOI\",], inplace = True) # Rows without abstracts\n",
    "combined_df.drop_duplicates(subset=[\"DOI\", \"Title\"], keep = \"first\", inplace = True) # Dropping duplicate DOI entries\n",
    "\n",
    "# Save the combined DataFrame as a single gzip file\n",
    "ALL_QUERIES_PATH = f\"{QUERY_DIR}/All_Queries.gzip\"\n",
    "combined_df.to_parquet(ALL_QUERIES_PATH, compression = \"gzip\", engine = \"pyarrow\")\n",
    "\n",
    "print(f\"Combined {len(gzip_files)} gzip files from subdirectories into {ALL_QUERIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27316318",
   "metadata": {},
   "source": [
    "# Abstract Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c426f90-eab2-4a42-8a5d-4d6dd21f510e",
   "metadata": {},
   "source": [
    "### Either use the code below or use batched_abstracts.py with the sbatch file submit_abstracts.sh for faster processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9302a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(ALL_QUERIES_PATH)\n",
    "doi_to_category = df.set_index(\"DOI\")[\"Plastic_Category\"].to_dict()\n",
    "doi_series = df_queries[\"DOI\"].reset_index(drop = True)  # Sequential index\n",
    "max_index = len(doi_series) - 1  # Maximum valid index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99fb700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Lists to store extracted data\n",
    "list_abstract = []\n",
    "list_doi = []\n",
    "list_title = []\n",
    "list_date = []\n",
    "list_journal = []\n",
    "list_category = []  # New list for plastic category\n",
    "# Run in small batches in case of error, e.g. 0-1000 at a time.\n",
    "start = 0 \n",
    "end = 5\n",
    "for i in tqdm(range(start, end + 1)):\n",
    "    try:\n",
    "        current_doi = doi_series[i]\n",
    "        doi_doc = FullDoc(doi=current_doi)\n",
    "        if doi_doc.read(client):\n",
    "            abstract = doi_doc._data[\"coredata\"][\"dc:description\"]\n",
    "            title = doi_doc.title\n",
    "            date = doi_doc._data[\"coredata\"][\"prism:coverDisplayDate\"]\n",
    "            journal = doi_doc._data[\"coredata\"][\"prism:publicationName\"]\n",
    "            \n",
    "            list_abstract.append(abstract)\n",
    "            list_doi.append(current_doi)\n",
    "            list_title.append(title)\n",
    "            list_date.append(date)\n",
    "            list_journal.append(journal)\n",
    "            \n",
    "            plastic_category = doi_to_category.get(current_doi, \"Unknown\")\n",
    "            list_category.append(plastic_category)\n",
    "\n",
    "        else:\n",
    "            print(f\"Operation failed for DOI: {current_doi}\")\n",
    "            # Append placeholders\n",
    "            list_abstract.append(\"None\")\n",
    "            list_doi.append(current_doi)\n",
    "            list_title.append(\"None\")\n",
    "            list_date.append(\"None\")\n",
    "            list_journal.append(\"None\")\n",
    "            list_category.append(\"None\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle cases where i might be invalid (though adjusted)\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        list_abstract.append(\"None\")\n",
    "        list_doi.append(doi_series[i] if i <= max_index else \"Invalid DOI\")\n",
    "        list_title.append(\"None\")\n",
    "        list_date.append(\"None\")\n",
    "        list_journal.append(\"None\")\n",
    "        list_category.append(\"None\")\n",
    "\n",
    "# Create and save the extracted data\n",
    "df_abstracts = pd.DataFrame({\n",
    "    \"DOI\": list_doi,\n",
    "    \"Title\": list_title,\n",
    "    \"Abstract\": list_abstract,\n",
    "    \"Date\": list_date,\n",
    "    \"Journal\": list_journal,\n",
    "    \"Plastic Category\": list_category})\n",
    "\n",
    "df_abstracts = df_abstracts.astype({\n",
    "    \"DOI\": \"str\",\n",
    "    \"Title\": \"str\",\n",
    "    \"Abstract\": \"str\",\n",
    "    \"Date\": \"str\",\n",
    "    \"Journal\": \"str\",\n",
    "    \"Plastic Category\": \"str\"\n",
    "})\n",
    "\n",
    "# Save abstracts\n",
    "ABSTRACT_DIR = f\"{QUERY_PATH}/Abstracts\"\n",
    "os.makedirs(ABSTRACT_DIR, exist_ok = True)\n",
    "ABSTRACT_PATH = f\"{ABSTRACT_DIR}/Abstracts_{start}_{end}.gzip\"\n",
    "df_abstracts.to_parquet(ABSTRACT_PATH, compression = \"gzip\", engine = \"pyarrow\")\n",
    "print(f\"Saved extracted abstracts from {start} to {end} in {ABSTRACT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954c8e5-086f-4af1-82da-3f2c7f77d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_abstracts_filename = \"All_Abstracts.gzip\"\n",
    "output_path = os.path.join(ABSTRACT_DIR, all_abstracts_filename)\n",
    "\n",
    "# Get all .gzip files in the directory, excluding the output file\n",
    "input_files = glob.glob(os.path.join(ABSTRACT_DIR, \"*.gzip\"))\n",
    "input_files = [f for f in input_files if f != output_path]\n",
    "\n",
    "# Sort files to ensure order (adjust if needed)\n",
    "input_files.sort()\n",
    "\n",
    "# Concatenate the files\n",
    "with open(output_path, \"wb\") as out_file:\n",
    "    for file_path in input_files:\n",
    "        with open(file_path, \"rb\") as in_file:\n",
    "            out_file.write(in_file.read())\n",
    "\n",
    "print(f\"Successfully concatenated {len(input_files)} files into {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACT_DIR = f\"{QUERY_PATH}/Abstracts\"\n",
    "os.makedirs(ABSTRACT_DIR, exist_ok = True)\n",
    "all_abstracts_path = f\"{ABSTRACT_DIR}\\All_Abstracts2.parquet.gzip\"\n",
    "\n",
    "# Get all .gzip files (assuming they are Parquet files)\n",
    "files = glob.glob(f\"{ABSTRACT_DIR}\\*.gzip\")\n",
    "\n",
    "# Exclude output file if it already exists\n",
    "files = [f for f in files if f != all_abstracts_path]\n",
    "\n",
    "# Read and concatenate Parquet files\n",
    "dfs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        df_part = pd.read_parquet(f)  # Read as Parquet\n",
    "        dfs.append(df_part)\n",
    "        print(f\"Successfully read: {f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED TO READ {f}: {str(e)}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No valid files to concatenate.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index = True)\n",
    "df.to_parquet(all_abstracts_path, compression = \"gzip\")\n",
    "print(f\"Merged {len(dfs)} files into {all_abstracts_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
